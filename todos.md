TODO:

- [x] Think about fuzzy matching for reduce / entity resolution
- [x] Support equijoins
- [x] Inputs should be accessed as input['title'] instead of just title / everything be jinja
- [x] Make flatten a separate operator with flatten_key (or nothing)
- [x] Convert parallel flatmap to parallel map
- [x] Write documentation & restructure codebase
- [x] Write tests
- [x] Chunking/splitting with peripheral chunks
- [x] Write build phase
- [x] Add keys / inputs to reduce
- [x] For reduce we should pass through keys
- [x] Optimize maps
  - [x] Track costs for the optimizer
  - [x] Generate multiple plans and evaluate them instead of generating one plan
  - [x] Don't use an LLM to determine the right chunk size; try searching many chunk sizes
  - [x] Call llm agent multiple times on different random inputs & average results
  - [ ] Decompose map to be a chain or parallel map
- [x] Optimize resolvers (add blocking rules)
- [ ] Optimize reduce
  - [x] Implement fold pattern
  - [x] Optimize folds
    - [x] Stratified sample the reduce operations based on the groupby results
    - [x] Synthesize multiple fold prompts
  - [x] Implement merge pattern
  - [ ] Optimize merges
    - [x] Synthesize merge prompts
    - [x] Derive num_parallel_folds in the reduce op itself (saving the runtimes of folds and merges)
    - [ ] Try various batch sizes
- [x] Optimize equijoins
- [ ] Auto-generate resolver
- [ ] Support multiple step workflows in the optimizer
- [ ] Write documentation on how all the operators work
- [ ] Change validation to be pairwise comparisons
- [ ] Write gleaning operations
- [ ] Support model pools
- [ ] Operator reordering
- [ ] Support passing expectations
- [ ] Write intermediates to disk
- [ ] Support order by
- [ ] Track traces
